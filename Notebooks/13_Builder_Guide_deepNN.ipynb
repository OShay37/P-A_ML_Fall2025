{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe757389",
   "metadata": {},
   "source": [
    "# Builder's Guide "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c177820",
   "metadata": {},
   "source": [
    "To get you this far this fast, we called upon the libraries, but skipped over more advanced details about how they work.\n",
    "\n",
    "These insights will move you from end user to power user, giving you the tools needed to reap the benefits of a mature deep learning library while retaining the flexibility to implement more complex models, including those you invent yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710e241",
   "metadata": {},
   "source": [
    "## Implementation of a Custom Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e88bde",
   "metadata": {},
   "source": [
    "Perhaps the easiest way to develop intuition about how a module works is to implement one ourselves. Before we do that, we briefly summarize the basic functionality that each module must provide:\n",
    "1. __Ingest input__ data as arguments to its forward propagation method.\n",
    "2. __Generate an output__ by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "3. __Calculate the gradient__ of its output with respect to its parameters, which can be accessed via its backpropagation method. Typically this happens automatically.\n",
    "4. __Store and provide access to those parameters__ necessary for executing the forward prop- agation computation.\n",
    "5. __Initialize model parameters__ as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a770fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "    \n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb617b24",
   "metadata": {},
   "source": [
    "Note that unless we implement a new layer, we need not worry about the backpropagation method or parameter initialization. The system will generate these methods automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a924c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 20)\n",
    "\n",
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b90c0",
   "metadata": {},
   "source": [
    "## The `nn.Sequential` Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9ff77",
   "metadata": {},
   "source": [
    "The following code generates a network with one fully connected hidden layer with 256 units and ReLU activation, followed by a fully connected output layer with ten units (no activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160219cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 20)\n",
    "\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c408dab",
   "metadata": {},
   "source": [
    "`nn.Sequential` defines a special kind of `Module`, the class that presents a module in PyTorch. It maintains an ordered list of constituent Modules. \n",
    "\n",
    "Note that each of the two fully connected layers is an instance of the `Linear` class which is itself a subclass of `Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa595b",
   "metadata": {},
   "source": [
    "Forward propagation (`forward`) method is also remarkably simple: it chains each module in the list together, passing the output of each as input to the next. Note that until now, we have been invoking our models via the construction `net(X)` to obtain their outputs. This is actually just shorthand for `net.__call__(X)`.\n",
    "\n",
    "\n",
    "But let's take a closer look by designing our own `MySequential` Module. We need to define two key methods:\n",
    "1. A method for appending modules one by one to a list.\n",
    "2. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fcc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd413c",
   "metadata": {},
   "source": [
    "In the __init__ method, we add every module by calling the `add_modules` method. These modules can be accessed by the children method at a later date. In this way the system knows the added modules, and it will properly initialize each module’s parameters.\n",
    "\n",
    "When our `MySequential`’s forward propagation method is invoked, each added module is executed in the order in which they were added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89853eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47cf5b4",
   "metadata": {},
   "source": [
    "## Constant parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba71368",
   "metadata": {},
   "source": [
    "You may have noticed that until now, all of the operations in our networks have acted upon our network’s activations and its parameters. Sometimes, however, we might want to incorporate terms that are neither the result of previous layers nor updatable parameters. We call these constant parameters.\n",
    "\n",
    "Say for example that we want a layer that calculates the function $f(x, w) = 𝑐 w^⊤ x $, where $x$ is the input, $w$ is our parameter, and 𝑐 is some specified constant that is not updated during optimization. So we implement a `FixedHiddenMLP` class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f78f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffdb45",
   "metadata": {},
   "source": [
    "In this model, we implement a hidden layer whose weights (self.rand_weight) are initialized randomly at instantiation and are thereafter constant. This weight is not a model parameter and thus it is never updated by backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353afbb",
   "metadata": {},
   "source": [
    "> __NOTE__ that before returning the output, our model ran a while-loop, testing on the condition its *l1* norm is larger than 1, and \n",
    "> dividing our output vector by 2 until it satisfied the condition. Finally, we returned the sum of the entries in X. \n",
    "> No standard neural network performs this operation. The point is only to show you how to integrate arbitrary code into the flow \n",
    "> of your neural network computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291e77a",
   "metadata": {},
   "source": [
    "## Mix and Nest Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507bcd7",
   "metadata": {},
   "source": [
    "We can mix and match various ways of assembling modules together. In the following example, we nest modules in some creative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39477fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668baeb",
   "metadata": {},
   "source": [
    "# Parameters Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48ac12",
   "metadata": {},
   "source": [
    "After training, we will need these parameters in order to make future predictions. Additionally, we will sometimes wish to \n",
    "- extract the parameters perhaps to reuse them in some other context, \n",
    "- to save our model to disk \n",
    "    - so that it may be executed in other software, \n",
    "    - or for examination in the hope of gaining scientific understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efa44d",
   "metadata": {},
   "source": [
    "## Accessing the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect the parameters of the second fully connected layer as follows.\n",
    "# We can see that this fully connected layer contains two parameters, corresponding to that layer’s weights and biases, respectively.\n",
    "\n",
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43d481",
   "metadata": {},
   "source": [
    "Note that each parameter is represented as an instance of the parameter class. To do anything useful with the parameters, we first need to access the underlying numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are complex objects, containing values, gradients, and additional information. That is why we need to request the value explicitly.\n",
    "\n",
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05410f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have not invoked backpropagation for this network yet, it is in its initial state (aka the gradient is set to None).\n",
    "\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08210720",
   "metadata": {},
   "source": [
    "When we need to perform operations on all parameters, accessing them one-by-one can grow tedious. \n",
    "\n",
    "The situation can grow especially unwieldy when we work with more complex, e.g., nested, modules, since we would need to recurse through the entire tree to extract each sub-module’s parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bfa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208fb86",
   "metadata": {},
   "source": [
    "Often, we want to share parameters across multiple layers. Let’s see how to do this elegantly. \n",
    "\n",
    "In the following we allocate a fully connected layer and then use its parameters specifically to set those of another layer. Here we need to run the forward propagation net(X) before accessing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc541879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the parameters for the shared layer\n",
    "net[2].weight.data[0, 0] = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that they are actually the same object rather than just having the same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4d83a",
   "metadata": {},
   "source": [
    "ATTENTION: They are not just equal, they are represented by the same exact tensor. Thus, if we change one of the parameters, the other one changes, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4175c",
   "metadata": {},
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3beb092",
   "metadata": {},
   "source": [
    "Now that we know how to access the parameters, let’s look at how to initialize them properly.\n",
    "\n",
    "The deep learning framework provides default random initializations to its layers. \n",
    "However, we often want to initialize our weights according to various other protocols. \n",
    "The framework provides most commonly used protocols, and also allows to create a custom initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab15371",
   "metadata": {},
   "source": [
    "By default, PyTorch initializes weight and bias matrices uniformly by drawing from a range that is computed according to the input and output dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d4f7a",
   "metadata": {},
   "source": [
    "## Built-in Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3254acc",
   "metadata": {},
   "source": [
    "PyTorch’s `nn.init` module provides a variety of preset initialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dda4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_constant(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.zeros_(module.bias)\n",
    "        \n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply different initializers for certain blocks.\n",
    "\n",
    "def init_xavier(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "def init_42(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30a1af",
   "metadata": {},
   "source": [
    "## Custom Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b9a79",
   "metadata": {},
   "source": [
    "In the example below, we define an initializer for any weight parameter $w$ using the following distribution:\n",
    "\n",
    "$$\n",
    "w \\sim \n",
    "\\begin{cases}\n",
    "U(5, 10), & \\text{with probability } \\tfrac{1}{4}, \\\\[6pt]\n",
    "0, & \\text{with probability } \\tfrac{1}{2}, \\\\[6pt]\n",
    "U(-10, -5), & \\text{with probability } \\tfrac{1}{4}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Again, we implement a `my_init` function to apply to `net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73426685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to understand this function think that the interval -10, 10 can be split in 4: (-10, -5) / (-5, 0) / (0, 5) / (5, 10)\n",
    "# if you sample randomly from this interval, you will get values in these ranges with equal probability\n",
    "# (module.weight.data.abs() >= 5) this returns a boolean with False (zeros) for values in (-5, 5) which is already 1/2 of the total interval. \n",
    "def my_init(module):\n",
    "\n",
    "    if type(module) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in module.named_parameters()][0])\n",
    "        nn.init.uniform_(module.weight, -10, 10)\n",
    "        module.weight.data =  module.weight.data * (module.weight.data.abs() >= 5) \n",
    "\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we always have the option of setting parameters directly.\n",
    "\n",
    "print(net[0].weight.data)\n",
    "\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82261b4b",
   "metadata": {},
   "source": [
    "## Lazy Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa422b86",
   "metadata": {},
   "source": [
    "So far, it might seem that we got away with being sloppy in setting up our networks. Specif- ically, we did the following unintuitive things, which might not seem like they should work:\n",
    "\n",
    "- We defined the network architectures without specifying the input dimensionality.\n",
    "- We added layers without specifying the output dimension of the previous layer.\n",
    "- We even “initialized” these parameters before providing enough information to determine how many parameters our models should contain.\n",
    "\n",
    "You might be surprised that our code runs at all. \n",
    "\n",
    "The trick here is that the framework defers initialization, waiting until the first time we pass data through the model, to infer the sizes of each layer on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb538e",
   "metadata": {},
   "source": [
    "At this point, the network cannot possibly know the dimensions of the input layer’s weights\n",
    "because the input dimension remains unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ce00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummmy input\n",
    "X = torch.rand(2, 20)\n",
    "\n",
    "# Run the dummy through teh net to infer the shapes of the parameters\n",
    "net(X)\n",
    "\n",
    "# Now we can inspect the shapes of the parameters\n",
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b774971",
   "metadata": {},
   "source": [
    "The following method passes in dummy inputs through the network for a dry run to infer all parameter shapes and subsequently initializes the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85219de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def apply_init(self, inputs, init=None):\n",
    "    self.forward(*inputs)\n",
    "    if init is not None:\n",
    "        self.net.apply(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3e045",
   "metadata": {},
   "source": [
    "Later on, when working with convolutional neural networks, this technique will become even more convenient since the input dimensionality (e.g., the resolution of an image) will affect the dimensionality of each subsequent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4a5e2",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0cb50",
   "metadata": {},
   "source": [
    "Sooner or later, you will need a layer that does not exist yet in the deep learning framework. In these cases, you must build a custom layer. \n",
    "\n",
    "The following `CenteredLayer` class simply subtracts the mean from its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s verify that our layer works as intended by feeding some data through it.\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35faef",
   "metadata": {},
   "source": [
    "We can now incorporate our layer as a component in constructing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2926a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e289f",
   "metadata": {},
   "source": [
    "Now let’s implement our own version of the fully connected layer. \n",
    "\n",
    "Recall that this layer requires two parameters, one to represent the weight and the other for the bias. In this implementation, we bake in the ReLU activation as a default. This layer requires two input arguments: in_units and units, which denote the number of inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can directly carry out forward propagation calculations using custom layers.\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also construct models using custom layers. Once we have that we can use it justlike the built-in fully connected layer.\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60adb891",
   "metadata": {},
   "source": [
    "# File I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe046c1",
   "metadata": {},
   "source": [
    "Oncce we are happy wth our model, we will want to save the results for later use in various contexts (perhaps even to make predictions in deployment). \n",
    "\n",
    "Additionally, when running a long training process, the best practice is to periodically save intermediate results (checkpointing) to ensure that we do not lose several days’ worth of computation if we trip over the power cord of our server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, '../data/x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the stored file back into memory.\n",
    "x2 = torch.load('../data/x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01321558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can store a list of tensors and read them back into memory.\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aad7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even write and read a dictionary that maps from strings to tensors.\n",
    "\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfddfcd",
   "metadata": {},
   "source": [
    "## Loading and Saving Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3f179",
   "metadata": {},
   "source": [
    "eep learning framework provides built-in functionalities to load and save entire networks. \n",
    "\n",
    "An important detail to note is that this saves model parameters and not the entire model. For example, if we have a 3-layer MLP, we need to specify the architecture separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "    \n",
    "    \n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone = MLP()\n",
    "\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c4403",
   "metadata": {},
   "source": [
    "Since both instances have the same model parameters, the computational result of the same input X should be the same. Let’s verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13076561",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d708e7",
   "metadata": {},
   "source": [
    "# GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c292461",
   "metadata": {},
   "source": [
    "In PyTorch, every array has a device; we often refer it as a *context*. So far, by default, all variables and associated computation have been assigned to the CPU. Typically, other contexts might be various GPUs. Things can get even hairier when we deploy jobs across multiple servers. \n",
    "By assigning arrays to contexts intelligently, we can minimize the time spent transferring data between devices. For example, when training neural networks on a server with a GPU, we typically prefer for the model’s parameters to live on the GPU.\n",
    "\n",
    "__To run the programs in this section, you need at least two GPUs.__\n",
    "\n",
    "#### Note -- Macs only have one GPU with a different architecture:\n",
    "\n",
    "Apple silicon GPUs use a unified memory architecture where the CPU and GPU share a single memory pool, providing high bandwidth and efficient data sharing, while traditional NVIDIA GPUs employ discrete memory (VRAM) for the GPU and separate RAM for the CPU. Key differences also lie in their respective graphics APIs (Metal for Apple, CUDA for NVIDIA), target applications (Apple silicon for integrated system efficiency, NVIDIA for raw performance in dedicated tasks), and power efficiency, with Apple silicon focusing on lower power consumption for its integrated system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu():  #@save\n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def gpu(i=0):  #@save\n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    \n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "cpu(), gpu(), gpu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_gpus():  #@save\n",
    "    if torch.backends.mps.is_available():\n",
    "        return 1  # Only 1 MPS GPU is available\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.cuda.device_count()\n",
    "\n",
    "num_gpus() # Macs won't have any NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if num_gpus() >= i + 1:\n",
    "        return gpu(i)\n",
    "    return cpu()\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    return [gpu(i) for i in range(num_gpus())]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a808021",
   "metadata": {},
   "source": [
    "By default, tensors are created on the CPU. We can query the device where the tensor is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebde0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X, X.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y, Y.device\n",
    "\n",
    "# In Macs with one GPU: this will be put on a CPU, but it won't matter because CPU and GPU on new Macs share the same memory pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_gpus() > 1:\n",
    "    # need to have both tensor on the same GPU\n",
    "    Z = X.cuda(1)\n",
    "    print(Z.device)\n",
    "else:\n",
    "    # For Macs: need to have both tensors on GPU\n",
    "    Z = X.to(try_gpu())\n",
    "    \n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117691d4",
   "metadata": {},
   "source": [
    "People use GPUs to do machine learning because they expect them to be fast. But transferring variables between devices is slow: much slower than computation. \n",
    "\n",
    "Transferring data is not only slow, it also makes parallelization a lot more difficult, since we have to wait for data to be sent (or rather to be received) before we can proceed with more operations. This is why copy operations should be taken with great care. \n",
    "\n",
    "As a rule of thumb, many small operations are much worse than one big operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6d70d",
   "metadata": {},
   "source": [
    "## NN and GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48822bad",
   "metadata": {},
   "source": [
    "Similarly, a neural network model can specify devices. The following code puts the model parameters on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(1))\n",
    "net = net.to(device=try_gpu())\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ef585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s confirm that the model parameters are stored on the same GPU.\n",
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283e4ea",
   "metadata": {},
   "source": [
    "Let the trainer support GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "    self.save_hyperparameters()\n",
    "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_batch(self, batch):\n",
    "    if self.gpus:\n",
    "        batch = [a.to(self.gpus[0]) for a in batch]\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_model(self, model):\n",
    "    model.trainer = self\n",
    "    model.board.xlim = [0, self.max_epochs]\n",
    "    if self.gpus:\n",
    "        model.to(self.gpus[0])\n",
    "    self.model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc058dd7",
   "metadata": {},
   "source": [
    "In short, as long as all data and parameters are on the same device, we can learn models efficiently. In the following chapters we will see several such examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
